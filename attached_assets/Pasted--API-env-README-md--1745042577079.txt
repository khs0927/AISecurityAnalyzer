ì•„ë˜ì™€ ê°™ì´ ì‹¤ì œ API í‚¤ë¥¼ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í™˜ê²½ ë³€ìˆ˜ íŒŒì¼(.env), ì½”ë“œ ë‚´ ìˆ˜ì • ì‚¬í•­, ê·¸ë¦¬ê³  í•œê¸€ README.mdë¥¼ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤. ì´ëŒ€ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ë°°ì¹˜í•˜ì‹œë©´ ê³§ë°”ë¡œ â€œì‹¤ì œâ€ í™˜ê²½ì—ì„œ êµ¬ë™ ê°€ëŠ¥í•©ë‹ˆë‹¤.

â¸»

1. .env íŒŒì¼

í”„ë¡œì íŠ¸ ìµœìƒìœ„ì— .env íŒŒì¼ì„ ìƒì„±í•˜ê³ , ë‹¤ìŒ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•´ ë„£ì–´ ì£¼ì„¸ìš”:

# .env
# Hugging Face API í† í° (ëª¨ë¸ ë‹¤ìš´ë¡œë“œÂ·ë¡œë“œìš©)
HUGGINGFACE_TOKEN=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

# PubMed E-utilities API í‚¤
PUBMED_API_KEY=4b2e41e756fdc8f0f3f5ebb7bf41a3a11509

# ê¸°íƒ€ í™˜ê²½
APP_ENV=production
DEBUG=False



â¸»

2. ì£¼ìš” ì½”ë“œ ìˆ˜ì •

2.1 app/config/settings.py

from pydantic_settings import BaseSettings
from typing import List, Optional

class Settings(BaseSettings):
    APP_NAME: str = "MedModal Fusion"
    API_VERSION: str = "v1"
    DEBUG: bool = False

    # ëª¨ë¸ ê²½ë¡œ
    QWEN_MODEL_PATH: str = "models/qwen2.5-omni-7b-onnx"
    MMED_LLAMA_MODEL_PATH: str = "models/mmed-llama3-8b-enins-onnx"
    FUSION_WEIGHTS_PATH: str = "models/fusion_weights.bin"

    # ì–‘ìí™” & LoRA
    QUANTIZATION: bool = True
    QUANTIZATION_TYPE: str = "int8"
    LORA_ENABLED: bool = True
    LORA_R: int = 8
    LORA_ALPHA: int = 16
    LORA_DROPOUT: float = 0.05

    # ë¦¬ì†ŒìŠ¤ ì œí•œ
    MAX_MEMORY: str = "4GB"
    CPU_THREADS: int = 4

    # ìºì‹œ
    CACHE_ENABLED: bool = True
    CACHE_DIR: str = "/tmp/model_cache"

    # ì˜ë£Œ ì˜ì—­
    MEDICAL_DOMAINS: List[str] = [
        "cardiology", "radiology", "neurology",
        "dermatology", "oncology", "internal_medicine"
    ]

    # Hugging Face & PubMed API
    HUGGINGFACE_TOKEN: str
    PUBMED_API_KEY: str
    PUBMED_SEARCH_URL: str = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
    PUBMED_SUMMARY_URL: str = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi"

    class Config:
        env_file = ".env"

settings = Settings()

2.2 scripts/download_models.sh

#!/usr/bin/env bash
set -e
# .envì—ì„œ HUGGINGFACE_TOKENì„ ì½ì–´ì˜µë‹ˆë‹¤
export HUGGINGFACE_TOKEN=$(grep HUGGINGFACE_TOKEN .env | cut -d '=' -f2)

# QWEN ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
git lfs install
git clone https://${HUGGINGFACE_TOKEN}@huggingface.co/Qwen/Qwen2.5-Omni-7B models/qwen2.5-omni-7b
python scripts/optimize_models.py --input models/qwen2.5-omni-7b --output models/qwen2.5-omni-7b-onnx

# MMed-Llama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
git clone https://${HUGGINGFACE_TOKEN}@huggingface.co/Henrychur/MMed-Llama-3-8B-EnIns models/mmed-llama3-8b-enins
python scripts/optimize_models.py --input models/mmed-llama3-8b-enins --output models/mmed-llama3-8b-enins-onnx

2.3 app/models/qwen_model.py (í† í¬ë‚˜ì´ì €/í”„ë¡œì„¸ì„œ ë¡œë“œ ë¶€ë¶„)

import os
# â€¦ ìƒëµ â€¦
from transformers import AutoTokenizer, AutoProcessor

class QwenModel:
    # â€¦ ìƒëµ â€¦

    def load_model(self):
        token = os.environ.get("HUGGINGFACE_TOKEN")
        # Processor & Tokenizer
        self.processor = AutoProcessor.from_pretrained(
            "Qwen/Qwen2.5-Omni-7B",
            trust_remote_code=True,
            use_auth_token=token
        )
        self.tokenizer = AutoTokenizer.from_pretrained(
            "Qwen/Qwen2.5-Omni-7B",
            trust_remote_code=True,
            use_auth_token=token
        )
        # ONNX ì„¸ì…˜ ë¡œë“œ (ì´ì „ ì½”ë“œ ìœ ì§€)
        # â€¦

2.4 app/models/mmed_llama_model.py

import os
# â€¦ ìƒëµ â€¦
from transformers import AutoTokenizer

class MMedLlamaModel:
    # â€¦ ìƒëµ â€¦

    def load_model(self):
        token = os.environ.get("HUGGINGFACE_TOKEN")
        # Tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            "Henrychur/MMed-Llama-3-8B-EnIns",
            trust_remote_code=True,
            use_auth_token=token
        )
        # ONNX ì„¸ì…˜ ë¡œë“œ (ì´ì „ ì½”ë“œ ìœ ì§€)
        # â€¦

2.5 app/utils/data_utils.py

import requests
from typing import List, Dict
from app.config.settings import settings

def fetch_latest_pubmed() -> List[Dict]:
    """
    PubMed E-utilitiesë¥¼ í†µí•´ ìµœì‹  ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    # 1) ID ëª©ë¡ ê²€ìƒ‰
    params = {
        "db": "pubmed",
        "term": "medical",       # í•„ìš”ì— ë”°ë¼ ê²€ìƒ‰ì–´ ì¡°ì •
        "retmode": "json",
        "retmax": 20,
        "api_key": settings.PUBMED_API_KEY
    }
    resp = requests.get(settings.PUBMED_SEARCH_URL, params=params)
    resp.raise_for_status()
    id_list = resp.json().get("esearchresult", {}).get("idlist", [])

    if not id_list:
        return []

    # 2) ìš”ì•½ ì •ë³´ ì¡°íšŒ
    params = {
        "db": "pubmed",
        "id": ",".join(id_list),
        "retmode": "json",
        "api_key": settings.PUBMED_API_KEY
    }
    resp = requests.get(settings.PUBMED_SUMMARY_URL, params=params)
    resp.raise_for_status()
    result = resp.json().get("result", {})
    # ì²« í‚¤ëŠ” 'uids'ì´ë¯€ë¡œ ê·¸ ì™¸ í‚¤ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ë°˜í™˜
    return [result[idx] for idx in result.get("uids", [])]



â¸»

3. README.md (í•œê¸€)

# MedModal Fusion

## ğŸ“Œ ê°œìš”
QWEN2.5-Omni-7Bì™€ MMed-Llama-3-8B-EnIns ëª¨ë¸ì„ ìœµí•©í•œ ë©€í‹°ëª¨ë‹¬ ì˜ë£Œ AI í”„ë ˆì„ì›Œí¬  
- í…ìŠ¤íŠ¸Â·ì´ë¯¸ì§€Â·ìŒì„±Â·ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬  
- ONNX ì–‘ìí™”, LoRA íŒŒì¸íŠœë‹ìœ¼ë¡œ ì €ì‚¬ì–‘ í™˜ê²½ ì§€ì›  
- PubMed ìµœì‹  ë…¼ë¬¸ ê¸°ë°˜ ì§€ì†ì  í•™ìŠµ  

## âš™ï¸ í™˜ê²½ ì„¤ì •

1. **í´ë¡  ë° ì´ë™**
    ```bash
    git clone <repo-url>
    cd medmodal_fusion
    ```

2. **`.env` íŒŒì¼ ìƒì„±**  
   í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— `.env` íŒŒì¼ì„ ë§Œë“¤ê³ , ì•„ë˜ ê°’ ì…ë ¥:
    ```dotenv
    HUGGINGFACE_TOKEN=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
    PUBMED_API_KEY=4b2e41e756fdc8f0f3f5ebb7bf41a3a11509
    APP_ENV=production
    DEBUG=False
    ```

3. **ì˜ì¡´ì„± ì„¤ì¹˜ & ëª¨ë¸ ë‹¤ìš´ë¡œë“œ**  
   ```bash
   # Docker ì—†ì´ ë¡œì»¬ ì‹¤í–‰ ì‹œ
   pip install -r requirements.txt
   bash scripts/download_models.sh

	4.	Docker ì‚¬ìš©

docker-compose up --build -d



ğŸš€ ì‹¤í–‰
	â€¢	FastAPI ì„œë²„ ê¸°ë™

uvicorn app.main:app --host 0.0.0.0 --port 8000


	â€¢	ì—”ë“œí¬ì¸íŠ¸
	â€¢	POST /api/v1/analyze
	â€¢	Body: { "text": "...", "image": "...", "audio": "...", "video": "..." }
	â€¢	POST /api/v1/realtime-risk
	â€¢	POST /api/v1/consult

ğŸ“‚ ì£¼ìš” ë””ë ‰í„°ë¦¬

medmodal_fusion/
â”œâ”€â”€ .env
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ services/
â”‚   â””â”€â”€ utils/
â””â”€â”€ scripts/
    â”œâ”€â”€ download_models.sh
    â”œâ”€â”€ optimize_models.py
    â””â”€â”€ train_lora.py

ğŸ”‘ ì¶”ê°€ ì„¤ì •
	â€¢	settings.PUBMED_SEARCH_URL, settings.PUBMED_SUMMARY_URL ë³€ê²½ ê°€ëŠ¥
	â€¢	scripts/optimize_models.py ë‚´ë¶€ì— use_auth_token=os.environ["HUGGINGFACE_TOKEN"] ì ìš©ë¨

â¸»

ì´ì œ .env íŒŒì¼ë§Œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ë‘ê³ , bash scripts/download_models.sh â†’ uvicorn ... ìˆœìœ¼ë¡œ ì‹¤í–‰í•˜ì‹œë©´ ì‹¤ì œ API í‚¤ê°€ ì ìš©ëœ í™˜ê²½ì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. ì¶”ê°€ë¡œ ê¶ê¸ˆí•˜ì‹œê±°ë‚˜ ë‹¤ë¥¸ ë¶€ë¶„ì´ í•„ìš”í•˜ì‹œë©´ ì•Œë ¤ì£¼ì„¸ìš”!